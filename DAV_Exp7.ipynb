{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanvi1618/DAV_exp_54/blob/main/DAV_Exp7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 7\n",
        "\n",
        "\n",
        "> **Aim:** Perform the steps involved in Text Analytics in Python & R\n",
        "\n",
        "> **Lab Objective:** To introduce the concept of text analytics and its applications.\n",
        "\n",
        ">**Lab Outcome:** Design Text Analytics Application on a given data set. (LO4)\n",
        "\n"
      ],
      "metadata": {
        "id": "IYtJt4Dje35_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task to be performed:**\n",
        "1. Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "2. Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "3. Perform the following experiments using Python & R\n",
        "* Tokenization (Sentence & Word)\n",
        "* Frequency Distribution\n",
        "* Remove stopwords & punctuations\n",
        "* Lexicon Normalization (Stemming, Lemmatization)\n",
        "* Part of Speech tagging\n",
        "* Named Entity Recognization\n",
        "* Scrape data from a website\n",
        "4. Prepare a document with the Aim, Tasks performed, Program, Output, and Conclusion."
      ],
      "metadata": {
        "id": "b-_GUx9EfeW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tools & Libraries to be explored:**\n",
        "* Python Libraries: nltk, scattertext, SpaCy, TextBlob, sklearn, pandas, numpy\n",
        "* R Libraries: shiny, tm, quanteda"
      ],
      "metadata": {
        "id": "vhJ7y2eyf1FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Text_mining'\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    first_paragraph = soup.find('p')\n",
        "\n",
        "    if first_paragraph:\n",
        "        print(first_paragraph.text)\n",
        "    else:\n",
        "        print('No <p> tags found on the website')\n",
        "else:\n",
        "    print('Failed to retrieve data from the website')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i26c2OgrFSls",
        "outputId": "0d13cac9-26f0-4bb3-fc26-c2b919eb69fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGYixbqCrajg",
        "outputId": "8e80963c-f231-4f8b-dc58-111d38b73cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = first_paragraph.text\n",
        "print(text)\n",
        "print(\"\\n Sentence tokenization: \\n\" , sent_tokenize(text))\n",
        "print(\"\\n Word tokenization: \\n\" , word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCX2edoqGhO",
        "outputId": "25dfd991-bf15-464a-ed40-6bf544aeb8f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n",
            "\n",
            " Sentence tokenization: \n",
            " ['Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text.', 'It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.', '\"[1] Written resources may include websites, books, emails, reviews, and articles.', 'High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning.', 'According to Hotho et al.', '(2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.', '[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output.', \"'High quality' in text mining usually refers to some combination of relevance, novelty, and interest.\", 'Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).']\n",
            "\n",
            " Word tokenization: \n",
            " ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(word_tokenize(text))\n",
        "print(fdist.most_common(2))\n",
        "\n",
        "print(\"Frequency of each word: \\n\")\n",
        "for word, freq in fdist.items():\n",
        "    print(f'{word}: {freq}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdhYq_zisSzI",
        "outputId": "62eb6d8b-5921-4c4d-a71b-af4557f479db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 22), ('text', 9)]\n",
            "Frequency of each word: \n",
            "\n",
            "Text: 2\n",
            "mining: 7\n",
            ",: 22\n",
            "text: 9\n",
            "data: 3\n",
            "(: 5\n",
            "TDM: 1\n",
            "): 5\n",
            "or: 1\n",
            "analytics: 1\n",
            "is: 2\n",
            "the: 8\n",
            "process: 3\n",
            "of: 9\n",
            "deriving: 2\n",
            "high-quality: 1\n",
            "information: 5\n",
            "from: 2\n",
            ".: 9\n",
            "It: 1\n",
            "involves: 2\n",
            "``: 2\n",
            "discovery: 2\n",
            "by: 4\n",
            "computer: 1\n",
            "new: 1\n",
            "previously: 1\n",
            "unknown: 1\n",
            "automatically: 1\n",
            "extracting: 1\n",
            "different: 2\n",
            "written: 1\n",
            "resources: 2\n",
            "[: 2\n",
            "1: 1\n",
            "]: 2\n",
            "Written: 1\n",
            "may: 1\n",
            "include: 2\n",
            "websites: 1\n",
            "books: 1\n",
            "emails: 1\n",
            "reviews: 1\n",
            "and: 9\n",
            "articles: 1\n",
            "High-quality: 1\n",
            "typically: 1\n",
            "obtained: 1\n",
            "devising: 1\n",
            "patterns: 2\n",
            "trends: 1\n",
            "means: 1\n",
            "such: 1\n",
            "as: 1\n",
            "statistical: 1\n",
            "pattern: 1\n",
            "learning: 2\n",
            "According: 1\n",
            "to: 2\n",
            "Hotho: 1\n",
            "et: 1\n",
            "al: 1\n",
            "2005: 1\n",
            "we: 1\n",
            "can: 1\n",
            "distinguish: 1\n",
            "between: 2\n",
            "three: 1\n",
            "perspectives: 1\n",
            ":: 1\n",
            "extraction: 2\n",
            "a: 2\n",
            "knowledge: 1\n",
            "in: 2\n",
            "databases: 1\n",
            "KDD: 1\n",
            "2: 1\n",
            "usually: 3\n",
            "structuring: 1\n",
            "input: 1\n",
            "parsing: 1\n",
            "along: 1\n",
            "with: 1\n",
            "addition: 1\n",
            "some: 2\n",
            "derived: 1\n",
            "linguistic: 1\n",
            "features: 1\n",
            "removal: 1\n",
            "others: 1\n",
            "subsequent: 1\n",
            "insertion: 1\n",
            "into: 1\n",
            "database: 1\n",
            "within: 1\n",
            "structured: 1\n",
            "finally: 1\n",
            "evaluation: 1\n",
            "interpretation: 1\n",
            "output: 1\n",
            "'High: 1\n",
            "quality: 1\n",
            "': 1\n",
            "refers: 1\n",
            "combination: 1\n",
            "relevance: 1\n",
            "novelty: 1\n",
            "interest: 1\n",
            "Typical: 1\n",
            "tasks: 1\n",
            "categorization: 1\n",
            "clustering: 1\n",
            "concept/entity: 1\n",
            "production: 1\n",
            "granular: 1\n",
            "taxonomies: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "document: 1\n",
            "summarization: 1\n",
            "entity: 1\n",
            "relation: 1\n",
            "modeling: 1\n",
            "i.e.: 1\n",
            "relations: 1\n",
            "named: 1\n",
            "entities: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5X5NEH5tLwA",
        "outputId": "fd7250a4-4466-432b-d7b3-c87b7310ff4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkvgDeYosAff",
        "outputId": "0577ecda-24e0-4890-9e17-3a871615fae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'those', 'during', 'an', 'be', 've', 'ours', 'nor', 'myself', 'while', \"shouldn't\", 'again', 'not', \"you're\", 'himself', 'yours', \"hasn't\", 'now', 'weren', 'in', 'all', \"you've\", 'wasn', 'don', 'because', 'itself', 'y', 'how', 'before', 'but', 'through', 'wouldn', \"weren't\", 'any', 'their', 'being', 'each', 'it', \"you'd\", 't', 'very', 'then', 'they', 'these', \"it's\", 'other', 'haven', 'shouldn', 'ourselves', 'he', 'whom', 'hers', 'which', 'after', 'mightn', \"needn't\", 'doesn', 'what', 'some', 'do', 's', \"you'll\", 'few', \"aren't\", 'when', 'did', \"should've\", 'under', 'can', \"mustn't\", 'she', 'its', 'yourself', \"that'll\", 'only', 'here', 'more', 'll', 'won', 'had', 'against', 'theirs', 'where', 'we', \"couldn't\", 'shan', 'into', 'me', \"hadn't\", \"don't\", 'from', 'the', 'no', 'is', 'so', 'them', 'over', 'mustn', 'just', 'as', 'hadn', \"won't\", 'between', 'why', 'above', 'further', 'below', 'needn', 'o', 'my', 'too', \"haven't\", 'about', 'was', 'having', 'her', 'aren', 'should', 'a', 'up', 'have', 'of', \"wasn't\", 'couldn', 'if', \"wouldn't\", 'you', 'm', 'on', 'both', 'this', \"she's\", 'once', 're', 'am', \"didn't\", 'at', 'been', 'to', 'd', 'ain', 'hasn', 'doing', 'didn', 'has', 'that', \"isn't\", 'down', 'are', 'isn', 'who', 'will', 'most', 'such', 'ma', 'by', 'him', 'themselves', 'off', 'yourselves', 'herself', 'than', \"doesn't\", 'your', 'until', 'same', \"shan't\", 'i', \"mightn't\", 'were', 'our', 'with', 'does', 'and', 'for', 'out', 'there', 'his', 'own', 'or'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_tokens=[]\n",
        "for w in word_tokenize(text):\n",
        "    if w not in stop_words:\n",
        "         filtered_tokens.append(w)\n",
        "\n",
        "print(\"Tokenized Words:\",word_tokenize(text))\n",
        "print(\"Filterd Tokens:\",filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsxIHb55tZ1w",
        "outputId": "a92ef07a-43e1-433f-ff04-82b8ee7667c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Words: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n",
            "Filterd Tokens: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', '.', 'It', 'involves', '``', 'discovery', 'computer', 'new', ',', 'previously', 'unknown', 'information', ',', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'articles', '.', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', '.', 'According', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'knowledge', 'discovery', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', ',', 'subsequent', 'insertion', 'database', ')', ',', 'deriving', 'patterns', 'within', 'structured', 'data', ',', 'finally', 'evaluation', 'interpretation', 'output', '.', \"'High\", 'quality', \"'\", 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', ',', 'novelty', ',', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'named', 'entities', ')', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuations=list(string.punctuation)\n",
        "\n",
        "filtered_tokens2=[]\n",
        "\n",
        "for i in filtered_tokens:\n",
        "    if i not in punctuations:\n",
        "        filtered_tokens2.append(i)\n",
        "\n",
        "print(\"Filterd Tokens After Removing Punctuations:\",filtered_tokens2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXXULDp4tQPG",
        "outputId": "29fcff1d-882c-45ca-f3ef-e6ef0afd071f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filterd Tokens After Removing Punctuations: ['Text', 'mining', 'text', 'data', 'mining', 'TDM', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', 'It', 'involves', '``', 'discovery', 'computer', 'new', 'previously', 'unknown', 'information', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '``', '1', 'Written', 'resources', 'may', 'include', 'websites', 'books', 'emails', 'reviews', 'articles', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', 'According', 'Hotho', 'et', 'al', '2005', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', 'information', 'extraction', 'data', 'mining', 'knowledge', 'discovery', 'databases', 'KDD', 'process', '2', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', 'usually', 'parsing', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', 'subsequent', 'insertion', 'database', 'deriving', 'patterns', 'within', 'structured', 'data', 'finally', 'evaluation', 'interpretation', 'output', \"'High\", 'quality', 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', 'novelty', 'interest', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', 'text', 'clustering', 'concept/entity', 'extraction', 'production', 'granular', 'taxonomies', 'sentiment', 'analysis', 'document', 'summarization', 'entity', 'relation', 'modeling', 'i.e.', 'learning', 'relations', 'named', 'entities']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "stemmed_words=[]\n",
        "\n",
        "for w in filtered_tokens2:\n",
        "     stemmed_words.append(ps.stem(w))\n",
        "\n",
        "print(\"Filtered Tokens After Removing Punctuations:\",filtered_tokens2)\n",
        "print(\"Stemmed Tokens:\",stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLJMnQH3tuLW",
        "outputId": "cd91d8b8-b119-4765-ba3b-acf428d8f83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens After Removing Punctuations: ['Text', 'mining', 'text', 'data', 'mining', 'TDM', 'text', 'analytics', 'process', 'deriving', 'high-quality', 'information', 'text', 'It', 'involves', '``', 'discovery', 'computer', 'new', 'previously', 'unknown', 'information', 'automatically', 'extracting', 'information', 'different', 'written', 'resources', '``', '1', 'Written', 'resources', 'may', 'include', 'websites', 'books', 'emails', 'reviews', 'articles', 'High-quality', 'information', 'typically', 'obtained', 'devising', 'patterns', 'trends', 'means', 'statistical', 'pattern', 'learning', 'According', 'Hotho', 'et', 'al', '2005', 'distinguish', 'three', 'different', 'perspectives', 'text', 'mining', 'information', 'extraction', 'data', 'mining', 'knowledge', 'discovery', 'databases', 'KDD', 'process', '2', 'Text', 'mining', 'usually', 'involves', 'process', 'structuring', 'input', 'text', 'usually', 'parsing', 'along', 'addition', 'derived', 'linguistic', 'features', 'removal', 'others', 'subsequent', 'insertion', 'database', 'deriving', 'patterns', 'within', 'structured', 'data', 'finally', 'evaluation', 'interpretation', 'output', \"'High\", 'quality', 'text', 'mining', 'usually', 'refers', 'combination', 'relevance', 'novelty', 'interest', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', 'text', 'clustering', 'concept/entity', 'extraction', 'production', 'granular', 'taxonomies', 'sentiment', 'analysis', 'document', 'summarization', 'entity', 'relation', 'modeling', 'i.e.', 'learning', 'relations', 'named', 'entities']\n",
            "Stemmed Tokens: ['text', 'mine', 'text', 'data', 'mine', 'tdm', 'text', 'analyt', 'process', 'deriv', 'high-qual', 'inform', 'text', 'it', 'involv', '``', 'discoveri', 'comput', 'new', 'previous', 'unknown', 'inform', 'automat', 'extract', 'inform', 'differ', 'written', 'resourc', '``', '1', 'written', 'resourc', 'may', 'includ', 'websit', 'book', 'email', 'review', 'articl', 'high-qual', 'inform', 'typic', 'obtain', 'devis', 'pattern', 'trend', 'mean', 'statist', 'pattern', 'learn', 'accord', 'hotho', 'et', 'al', '2005', 'distinguish', 'three', 'differ', 'perspect', 'text', 'mine', 'inform', 'extract', 'data', 'mine', 'knowledg', 'discoveri', 'databas', 'kdd', 'process', '2', 'text', 'mine', 'usual', 'involv', 'process', 'structur', 'input', 'text', 'usual', 'pars', 'along', 'addit', 'deriv', 'linguist', 'featur', 'remov', 'other', 'subsequ', 'insert', 'databas', 'deriv', 'pattern', 'within', 'structur', 'data', 'final', 'evalu', 'interpret', 'output', \"'high\", 'qualiti', 'text', 'mine', 'usual', 'refer', 'combin', 'relev', 'novelti', 'interest', 'typic', 'text', 'mine', 'task', 'includ', 'text', 'categor', 'text', 'cluster', 'concept/ent', 'extract', 'product', 'granular', 'taxonomi', 'sentiment', 'analysi', 'document', 'summar', 'entiti', 'relat', 'model', 'i.e.', 'learn', 'relat', 'name', 'entiti']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lemmatized Text:\", lemmatized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Y5XfCsuKpg",
        "outputId": "21e78fe0-f15b-4730-a206-d9a9e3010b37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\n",
            "\n",
            "Lemmatized Text: text mining , text data mining ( TDM ) or text analytic be the process of derive high - quality information from text . it involve \" the discovery by computer of new , previously unknown information , by automatically extract information from different write resource . \"[1 ] write resource may include website , book , email , review , and article . high - quality information be typically obtain by devise pattern and trend by mean such as statistical pattern learn . accord to Hotho et al . ( 2005 ) we can distinguish between three different perspective of text mining : information extraction , data mining , and a knowledge discovery in database ( KDD ) process.[2 ] text mining usually involve the process of structure the input text ( usually parse , along with the addition of some derive linguistic feature and the removal of other , and subsequent insertion into a database ) , derive pattern within the structured datum , and finally evaluation and interpretation of the output . ' high quality ' in text mining usually refer to some combination of relevance , novelty , and interest . typical text mining task include text categorization , text clustering , concept / entity extraction , production of granular taxonomy , sentiment analysis , document summarization , and entity relation modeling ( i.e. , learn relation between name entity ) . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbOsejsfvVlW",
        "outputId": "d83d65a2-b49c-4042-d9af-cbfa23b5e49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "tokens=word_tokenize(text)\n",
        "pos_=pos_tag(tokens)\n",
        "\n",
        "\n",
        "print(\"Tokens:\",tokens)\n",
        "print(\"PoS tags:\",pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItTkpCDvHMb",
        "outputId": "2629603e-72fd-4d9f-a755-516dc49c7470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Text', 'mining', ',', 'text', 'data', 'mining', '(', 'TDM', ')', 'or', 'text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'high-quality', 'information', 'from', 'text', '.', 'It', 'involves', '``', 'the', 'discovery', 'by', 'computer', 'of', 'new', ',', 'previously', 'unknown', 'information', ',', 'by', 'automatically', 'extracting', 'information', 'from', 'different', 'written', 'resources', '.', '``', '[', '1', ']', 'Written', 'resources', 'may', 'include', 'websites', ',', 'books', ',', 'emails', ',', 'reviews', ',', 'and', 'articles', '.', 'High-quality', 'information', 'is', 'typically', 'obtained', 'by', 'devising', 'patterns', 'and', 'trends', 'by', 'means', 'such', 'as', 'statistical', 'pattern', 'learning', '.', 'According', 'to', 'Hotho', 'et', 'al', '.', '(', '2005', ')', 'we', 'can', 'distinguish', 'between', 'three', 'different', 'perspectives', 'of', 'text', 'mining', ':', 'information', 'extraction', ',', 'data', 'mining', ',', 'and', 'a', 'knowledge', 'discovery', 'in', 'databases', '(', 'KDD', ')', 'process', '.', '[', '2', ']', 'Text', 'mining', 'usually', 'involves', 'the', 'process', 'of', 'structuring', 'the', 'input', 'text', '(', 'usually', 'parsing', ',', 'along', 'with', 'the', 'addition', 'of', 'some', 'derived', 'linguistic', 'features', 'and', 'the', 'removal', 'of', 'others', ',', 'and', 'subsequent', 'insertion', 'into', 'a', 'database', ')', ',', 'deriving', 'patterns', 'within', 'the', 'structured', 'data', ',', 'and', 'finally', 'evaluation', 'and', 'interpretation', 'of', 'the', 'output', '.', \"'High\", 'quality', \"'\", 'in', 'text', 'mining', 'usually', 'refers', 'to', 'some', 'combination', 'of', 'relevance', ',', 'novelty', ',', 'and', 'interest', '.', 'Typical', 'text', 'mining', 'tasks', 'include', 'text', 'categorization', ',', 'text', 'clustering', ',', 'concept/entity', 'extraction', ',', 'production', 'of', 'granular', 'taxonomies', ',', 'sentiment', 'analysis', ',', 'document', 'summarization', ',', 'and', 'entity', 'relation', 'modeling', '(', 'i.e.', ',', 'learning', 'relations', 'between', 'named', 'entities', ')', '.']\n",
            "PoS tags: [('Text', 'NNP'), ('mining', 'NN'), (',', ','), ('text', 'NN'), ('data', 'NNS'), ('mining', 'NN'), ('(', '('), ('TDM', 'NNP'), (')', ')'), ('or', 'CC'), ('text', 'JJ'), ('analytics', 'NNS'), ('is', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('deriving', 'VBG'), ('high-quality', 'JJ'), ('information', 'NN'), ('from', 'IN'), ('text', 'NN'), ('.', '.'), ('It', 'PRP'), ('involves', 'VBZ'), ('``', '``'), ('the', 'DT'), ('discovery', 'NN'), ('by', 'IN'), ('computer', 'NN'), ('of', 'IN'), ('new', 'JJ'), (',', ','), ('previously', 'RB'), ('unknown', 'JJ'), ('information', 'NN'), (',', ','), ('by', 'IN'), ('automatically', 'RB'), ('extracting', 'VBG'), ('information', 'NN'), ('from', 'IN'), ('different', 'JJ'), ('written', 'VBN'), ('resources', 'NNS'), ('.', '.'), ('``', '``'), ('[', 'JJ'), ('1', 'CD'), (']', 'NNS'), ('Written', 'NNP'), ('resources', 'NNS'), ('may', 'MD'), ('include', 'VB'), ('websites', 'NNS'), (',', ','), ('books', 'NNS'), (',', ','), ('emails', 'NNS'), (',', ','), ('reviews', 'NNS'), (',', ','), ('and', 'CC'), ('articles', 'NNS'), ('.', '.'), ('High-quality', 'NN'), ('information', 'NN'), ('is', 'VBZ'), ('typically', 'RB'), ('obtained', 'VBN'), ('by', 'IN'), ('devising', 'VBG'), ('patterns', 'NNS'), ('and', 'CC'), ('trends', 'NNS'), ('by', 'IN'), ('means', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('statistical', 'JJ'), ('pattern', 'NN'), ('learning', 'NN'), ('.', '.'), ('According', 'VBG'), ('to', 'TO'), ('Hotho', 'NNP'), ('et', 'FW'), ('al', 'NN'), ('.', '.'), ('(', '('), ('2005', 'CD'), (')', ')'), ('we', 'PRP'), ('can', 'MD'), ('distinguish', 'VB'), ('between', 'IN'), ('three', 'CD'), ('different', 'JJ'), ('perspectives', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('mining', 'NN'), (':', ':'), ('information', 'NN'), ('extraction', 'NN'), (',', ','), ('data', 'NN'), ('mining', 'NN'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('knowledge', 'NN'), ('discovery', 'NN'), ('in', 'IN'), ('databases', 'NNS'), ('(', '('), ('KDD', 'NNP'), (')', ')'), ('process', 'NN'), ('.', '.'), ('[', 'CC'), ('2', 'CD'), (']', 'JJ'), ('Text', 'NNP'), ('mining', 'NN'), ('usually', 'RB'), ('involves', 'VBZ'), ('the', 'DT'), ('process', 'NN'), ('of', 'IN'), ('structuring', 'VBG'), ('the', 'DT'), ('input', 'NN'), ('text', 'NN'), ('(', '('), ('usually', 'RB'), ('parsing', 'VBG'), (',', ','), ('along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('addition', 'NN'), ('of', 'IN'), ('some', 'DT'), ('derived', 'VBN'), ('linguistic', 'JJ'), ('features', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('removal', 'NN'), ('of', 'IN'), ('others', 'NNS'), (',', ','), ('and', 'CC'), ('subsequent', 'JJ'), ('insertion', 'NN'), ('into', 'IN'), ('a', 'DT'), ('database', 'NN'), (')', ')'), (',', ','), ('deriving', 'VBG'), ('patterns', 'NNS'), ('within', 'IN'), ('the', 'DT'), ('structured', 'JJ'), ('data', 'NNS'), (',', ','), ('and', 'CC'), ('finally', 'RB'), ('evaluation', 'NN'), ('and', 'CC'), ('interpretation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('output', 'NN'), ('.', '.'), (\"'High\", 'JJ'), ('quality', 'NN'), (\"'\", \"''\"), ('in', 'IN'), ('text', 'JJ'), ('mining', 'NN'), ('usually', 'RB'), ('refers', 'VBZ'), ('to', 'TO'), ('some', 'DT'), ('combination', 'NN'), ('of', 'IN'), ('relevance', 'NN'), (',', ','), ('novelty', 'NN'), (',', ','), ('and', 'CC'), ('interest', 'NN'), ('.', '.'), ('Typical', 'JJ'), ('text', 'NN'), ('mining', 'NN'), ('tasks', 'NNS'), ('include', 'VBP'), ('text', 'JJ'), ('categorization', 'NN'), (',', ','), ('text', 'NN'), ('clustering', 'NN'), (',', ','), ('concept/entity', 'NN'), ('extraction', 'NN'), (',', ','), ('production', 'NN'), ('of', 'IN'), ('granular', 'JJ'), ('taxonomies', 'NNS'), (',', ','), ('sentiment', 'NN'), ('analysis', 'NN'), (',', ','), ('document', 'NN'), ('summarization', 'NN'), (',', ','), ('and', 'CC'), ('entity', 'NN'), ('relation', 'NN'), ('modeling', 'NN'), ('(', '('), ('i.e.', 'FW'), (',', ','), ('learning', 'VBG'), ('relations', 'NNS'), ('between', 'IN'), ('named', 'VBN'), ('entities', 'NNS'), (')', ')'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4bnpBZiwcDv",
        "outputId": "3ae21566-2bb6-42ed-8c54-dbe7ff0024d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1tzFemLG72t",
        "outputId": "afb0ed91-81b9-49af-9736-424f68f32b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "\n",
        "for chunk in ne_chunk(nltk.pos_tag(word_tokenize(text))):\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md5O9RbLwILY",
        "outputId": "9c2aa5a6-a16a-489e-84ed-064671d933a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE Text\n",
            "ORGANIZATION TDM\n",
            "GPE Hotho\n",
            "ORGANIZATION KDD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"tokenizers\")\n",
        "install.packages(\"tm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9cCJyKnyNqI",
        "outputId": "fe026154-fe22-4543-eafe-c4f0d5e329d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text analytics in R\n",
        "\n",
        "library(tokenizers)\n",
        "\n",
        "text <- readline(prompt = \"Enter text: \")\n",
        "\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "sentence_tokens <- unlist(tokenize_sentences(text))\n",
        "\n",
        "cat(\"\\nTokenized words:\\n\")\n",
        "print(word_tokens)\n",
        "\n",
        "cat(\"\\nTokenized sentences:\\n\")\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkuOuDriyB9R",
        "outputId": "d0c71ff4-bb3b-4c80-ec21-0d3fce0123be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: Tokenization is the first step in text analytics. The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization. The token is a single entity that is building blocks for sentences or paragraphs.\n",
            "\n",
            "Tokenized words:\n",
            " [1] \"tokenization\" \"is\"           \"the\"          \"first\"        \"step\"        \n",
            " [6] \"in\"           \"text\"         \"analytics\"    \"the\"          \"process\"     \n",
            "[11] \"of\"           \"breaking\"     \"down\"         \"text\"         \"paragraphs\"  \n",
            "[16] \"into\"         \"smaller\"      \"chunks\"       \"such\"         \"as\"          \n",
            "[21] \"words\"        \"or\"           \"sentences\"    \"is\"           \"called\"      \n",
            "[26] \"tokenization\" \"the\"          \"token\"        \"is\"           \"a\"           \n",
            "[31] \"single\"       \"entity\"       \"that\"         \"is\"           \"building\"    \n",
            "[36] \"blocks\"       \"for\"          \"sentences\"    \"or\"           \"paragraphs\"  \n",
            "\n",
            "Tokenized sentences:\n",
            "[1] \"Tokenization is the first step in text analytics.\"                                                                  \n",
            "[2] \"The process of breaking down text paragraphs into smaller chunks such as words or sentences is called Tokenization.\"\n",
            "[3] \"The token is a single entity that is building blocks for sentences or paragraphs.\"                                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(head(sort(word_freq, decreasing = TRUE), 2))\n",
        "\n",
        "print(\"Frequency of each word:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbNstMQK1Xsn",
        "outputId": "ffd4e203-b1b1-4229-f5d5-d56740afe391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Most common words:\"\n",
            "word_tokens\n",
            " is the \n",
            "  4   3 \n",
            "[1] \"Frequency of each word:\"\n",
            "word_tokens\n",
            "           a    analytics           as       blocks     breaking     building \n",
            "           1            1            1            1            1            1 \n",
            "      called       chunks         down       entity        first          for \n",
            "           1            1            1            1            1            1 \n",
            "          in         into           is           of           or   paragraphs \n",
            "           1            1            4            1            2            2 \n",
            "     process    sentences       single      smaller         step         such \n",
            "           1            2            1            1            1            1 \n",
            "        text         that          the        token tokenization        words \n",
            "           2            1            3            1            2            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tm)\n",
        "\n",
        "filtered_tokens <- word_tokens[!word_tokens %in% stopwords(\"en\")]\n",
        "\n",
        "print(\"Filtered Tokens:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jck6zCh91n9w",
        "outputId": "e5e3cfee-b7c7-45f8-ee8f-68bb71ebc326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered Tokens:\"\n",
            " [1] \"tokenization\" \"first\"        \"step\"         \"text\"         \"analytics\"   \n",
            " [6] \"process\"      \"breaking\"     \"text\"         \"paragraphs\"   \"smaller\"     \n",
            "[11] \"chunks\"       \"words\"        \"sentences\"    \"called\"       \"tokenization\"\n",
            "[16] \"token\"        \"single\"       \"entity\"       \"building\"     \"blocks\"      \n",
            "[21] \"sentences\"    \"paragraphs\"  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, stemDocument)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "stemmed_corpus <- stemming(filtered_tokens)\n",
        "\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(unlist(sapply(stemmed_corpus, as.character)))"
      ],
      "metadata": {
        "id": "ycBxecfP2PoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ca571e7-34f7-4ee1-be95-93cde0ac134c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, stemDocument):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed Tokens:\"\n",
            " [1] \"token\"     \"first\"     \"step\"      \"text\"      \"analyt\"    \"process\"  \n",
            " [7] \"break\"     \"text\"      \"paragraph\" \"smaller\"   \"chunk\"     \"word\"     \n",
            "[13] \"sentenc\"   \"call\"      \"token\"     \"token\"     \"singl\"     \"entiti\"   \n",
            "[19] \"build\"     \"block\"     \"sentenc\"   \"paragraph\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatization <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, lemmatize_strings)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "lemmatized_corpus <- lemmatization(text)\n",
        "\n",
        "print(\"Lemmatized Tokens:\")\n",
        "print(unlist(sapply(lemmatized_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgvtDhni5Lr2",
        "outputId": "42f4157c-3798-479b-d063-8c9c2c04b8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, lemmatize_strings):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Lemmatized Tokens:\"\n",
            "[1] \"Token is the first step in text analytics. The process of break down text paragraph into smaller chunk such as word or sentenc is call Tokenization. The token is a singl entiti that is build block for sentenc or paragraphs.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(rvest)\n",
        "\n",
        "url <- 'https://en.wikipedia.org/wiki/Text_mining'\n",
        "\n",
        "page <- read_html(url)\n",
        "\n",
        "if (!is.null(page)) {\n",
        "  print(page)\n",
        "\n",
        "  first_paragraph <- html_nodes(page, 'p')[1]\n",
        "\n",
        "  if (!is.null(first_paragraph)) {\n",
        "    print(html_text(first_paragraph))\n",
        "  } else {\n",
        "    print('No <p> tags found on the website')\n",
        "  }\n",
        "} else {\n",
        "  print('Failed to retrieve data from the website')\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpYTH0duHjvn",
        "outputId": "58db0a43-a681-42e0-9c8e-d51807a70764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{html_document}\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-feature-night-mode-disabled skin-night-mode-clientpref-0 vector-toc-available\" lang=\"en\" dir=\"ltr\">\n",
            "[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n",
            "[2] <body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n",
            "[1] \"Text mining, text data mining (TDM) or text analytics is the process of deriving high-quality information from text. It involves \\\"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\\\"[1] Written resources may include websites, books, emails, reviews, and articles. High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can distinguish between three different perspectives of text mining: information extraction, data mining, and a knowledge discovery in databases (KDD) process.[2] Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps involved in text analytics\n",
        "\n",
        "1. **Data gathering:**\n",
        "In this stage, you gather text data from internal or external sources.\n",
        "> Internal data: Internal data is text content that is internal to your business and is readily available—for example, emails, chats, invoices, and employee surveys.\n",
        "\n",
        " > External data:  You can find external data in sources such as social media posts, online reviews, news articles, and online forums. It is harder to acquire external data because it is beyond your control. You might need to use web scraping tools or integrate with third-party solutions to extract external data.\n",
        "\n",
        "2. **Data preparation:**\n",
        "Data preparation is an essential part of text analysis. It involves structuring raw text data in an acceptable format for analysis. The text analysis software automates the process and involves the following common natural language processing (NLP) methods.\n",
        "> Tokenization: Tokenization is segregating the raw text into multiple parts that make semantic sense. For example, the phrase text analytics benefits businesses tokenizes to the words text, analytics, benefits, and businesses.\n",
        "\n",
        " > Part-of-speech tagging: Part-of-speech tagging assigns grammatical tags to the tokenized text. For example, applying this step to the previously mentioned tokens results in text: Noun; analytics: Noun; benefits: Verb; businesses: Noun.\n",
        "\n",
        " >Parsing : Parsing establishes meaningful connections between the tokenized words with English grammar. It helps the text analysis software visualize the relationship between words.\n",
        "\n",
        " >Lemmatization: Lemmatization is a linguistic process that simplifies words into their dictionary form, or lemma. For example, the dictionary form of visualizing is visualize.\n",
        "\n",
        " >Stop words removal: Stop words are words that offer little or no semantic context to a sentence, such as and, or, and for. Depending on the use case, the software might remove them from the structured text.\n",
        "\n",
        "3. **Text analysis:**\n",
        "Text analysis is the core part of the process, in which text analysis software processes the text by using different methods.\n",
        "> Text classification: Classification is the process of assigning tags to the text data that are based on rules or machine learning-based systems.\n",
        "\n",
        " >Text extraction: Extraction involves identifying the presence of specific keywords in the text and associating them with tags. The software uses methods such as regular expressions and conditional random fields (CRFs) to do this.\n",
        "\n",
        "4. **Visualization:**\n",
        "Visualization is about turning the text analysis results into an easily understandable format. You will find text analytics results in graphs, charts, and tables. The visualized results help you identify patterns and trends and build action plans. For example, suppose you’re getting a spike in product returns, but you have trouble finding the causes. With visualization, you look for words such as defects, wrong size, or not a good fit in the feedback and tabulate them into a chart. Then you’ll know which is the major issue that takes top priority."
      ],
      "metadata": {
        "id": "ZUo2ZcfdkdOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benefits of Text Analytics\n",
        "* Helps in understanding emerging customer trends, product performance, and service quality.\n",
        "* Helps researchers to explore pre-existing literature and extracting what’s relevant to their study.\n",
        "* Text analytic techniques help search engines to improve their performance, thereby providing fast user experiences.\n",
        "* Helps in making more data-driven decisions\n",
        "* Refines user content recommendation systems by categorizing related content\n",
        "* Boost Efficiency of working with Unstructured data"
      ],
      "metadata": {
        "id": "bQqgY5Qrlt8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Conclusion:\n",
        "* Identified the Text Analytics Libraries in Python and R\n",
        "* Performed simple experiments with these libraries in Python and R"
      ],
      "metadata": {
        "id": "j-rToYA0yifH"
      }
    }
  ]
}